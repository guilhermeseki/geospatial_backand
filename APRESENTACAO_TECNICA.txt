PLATAFORMA DE MONITORAMENTO CLIMÁTICO - APRESENTAÇÃO TÉCNICA (5 MINUTOS)

=================================================================================
SLIDE 1 - TÍTULO
=================================================================================

PLATAFORMA DE MONITORAMENTO CLIMÁTICO
Arquitetura, Dados e Capacidades Técnicas

=================================================================================
SLIDE 2 - VISÃO GERAL DO SISTEMA
=================================================================================

ARQUITETURA DE ALTO NÍVEL

COMPONENTES PRINCIPAIS:

1. INGESTÃO DE DADOS (Prefect Flows)
   • Pipelines automatizados para cada fonte de dados
   • Downloads via APIs oficiais (CDS, NOAA, NASA, ESA)
   • Processamento paralelo com Dask
   • Verificação de integridade e qualidade dos dados

2. ARMAZENAMENTO DUAL
   • GeoTIFF diários: Para visualização via GeoServer (WMS)
   • NetCDF histórico: Para queries rápidas via xarray
   • Compressão otimizada (zlib level 4)
   • Chunking estratégico (time=1, lat=20, lon=20)

3. PROCESSAMENTO E ANÁLISE
   • Shared Dask cluster (4 workers, 6GB cada)
   • Xarray para operações sobre arrays multidimensionais
   • Rasterio/GDAL para manipulação de GeoTIFFs
   • Shapely para operações geométricas (polígonos)

4. API REST (FastAPI)
   • Endpoints assíncronos (asyncio)
   • Queries executadas em thread pool (não bloqueiam event loop)
   • Documentação automática OpenAPI/Swagger
   • Validação de schemas com Pydantic

5. VISUALIZAÇÃO GEOESPACIAL
   • GeoServer para mapas WMS time-enabled
   • ImageMosaic stores com dimensão temporal
   • SLD styling customizado por variável
   • Cache automático de tiles

FLUXO DE DADOS:
Raw Data (APIs) → Prefect Flows → Dual Storage → Climate Data Service → FastAPI → Cliente

=================================================================================
SLIDE 3 - FONTES DE DADOS E CARACTERÍSTICAS
=================================================================================

PRECIPITAÇÃO

CHIRPS (Climate Hazards Group InfraRed Precipitation with Station data)
• Fonte: UCSB/USGS/NASA
• Resolução espacial: 0.05° (~5.5 km)
• Resolução temporal: Diária
• Cobertura: 50°S - 50°N (global tropical/subtropical)
• Histórico: 1981 - presente
• Latência: T+2 dias
• Método: Combina infravermelho de satélite + estações pluviométricas
• Formato original: GeoTIFF
• API: CHIRPS FTP/HTTP

MERGE (Merged Precipitation Product)
• Fonte: INPE (Instituto Nacional de Pesquisas Espaciais)
• Resolução espacial: 0.1° (~10 km)
• Resolução temporal: Diária
• Cobertura: América do Sul
• Histórico: 2000 - presente
• Latência: T+1 dia
• Método: Fusão satélite GOES + radar + estações brasileiras
• Formato original: Binary (convertido para GeoTIFF)
• Vantagem: Alta densidade de estações pluviométricas no Brasil

---

TEMPERATURA E VENTO

ERA5-Land (ECMWF Reanalysis v5)
• Fonte: ECMWF (European Centre for Medium-Range Weather Forecasts)
• Resolução espacial: 0.1° (~9 km)
• Resolução temporal: Horária (agregada para diária)
• Cobertura: Global
• Histórico: 1950 - presente
• Latência: T+5 dias
• Variáveis extraídas:
  - 2m_temperature (média, máxima, mínima diárias)
  - 10m_u_component_of_wind
  - 10m_v_component_of_wind (calculamos velocidade e direção)
• Método: Assimilação de dados de múltiplas fontes (satélites, estações, boias, aviões)
• API: Copernicus Climate Data Store (CDS)
• Formato original: NetCDF (GRIB convertido)

---

RAIOS

GLM (Geostationary Lightning Mapper)
• Fonte: NOAA/NASA (satélite GOES-16)
• Resolução espacial: 8-10 km no nadir
• Resolução temporal: 2 milissegundos
• Cobertura: Hemisfério oeste completo (contínuo)
• Histórico: 2017 - presente
• Latência: Tempo real (~30 segundos)
• Método: Detecção óptica de mudanças súbitas de brilho
• Formato original: NetCDF Level 2
• Processamento: Agregação em grid regular 0.1° com acumulação 30min
• Dados: Flash extent density (flashes/km²/30min)
• API: NOAA AWS S3 (públic bucket: noaa-goes16)

---

VEGETAÇÃO (NDVI)

Sentinel-2 L2A (Surface Reflectance)
• Fonte: ESA (European Space Agency)
• Resolução espacial: 10 m (bandas NIR e Red)
• Resolução temporal: 5 dias (2 satélites)
• Cobertura: Global terrestre
• Histórico: 2017 - presente
• Latência: T+2 dias
• Bandas usadas: B04 (Red, 665nm), B08 (NIR, 842nm)
• Cálculo: NDVI = (NIR - Red) / (NIR + Red)
• API: Microsoft Planetary Computer STAC
• Processamento: Mosaico de tiles, máscara de nuvens (SCL band)

MODIS MOD13Q1 (Vegetation Indices)
• Fonte: NASA
• Resolução espacial: 250 m
• Resolução temporal: 16 dias (composição)
• Cobertura: Global terrestre
• Histórico: 2000 - presente
• Latência: T+8 dias
• Produto: NDVI pré-calculado (composição máxima 16 dias)
• API: Microsoft Planetary Computer STAC
• Vantagem: Série temporal longa e consistente

=================================================================================
SLIDE 4 - ARQUITETURA DE ARMAZENAMENTO DUAL
=================================================================================

POR QUE DUAL STORAGE?

PROBLEMA:
• WMS (mapas) precisa de arquivos individuais por data (GeoTIFF)
• Queries temporais (séries históricas) são lentas em arquivos individuais
• NetCDF único é rápido para queries, mas GeoServer não suporta bem time dimension

SOLUÇÃO: Manter ambos os formatos sincronizados

---

FORMATO 1: GeoTIFF DIÁRIOS (Para GeoServer WMS)

Estrutura de diretórios:
/mnt/workwork/geoserver_data/
├── chirps/chirps_20250101.tif, chirps_20250102.tif, ...
├── merge/merge_20250101.tif, ...
├── temp_max/temp_max_20250101.tif, ...
├── temp_min/temp_min_20250101.tif, ...
├── temp/temp_20250101.tif, ...
├── ndvi_s2/ndvi_s2_20250101.tif, ...
└── ndvi_modis/ndvi_modis_20250101.tif, ...

Características:
• 1 arquivo = 1 data
• CRS: EPSG:4326 (WGS84)
• Bbox: LatAm (-94°W to -34°W, -56°S to 25°N)
• Compressão: LZW
• NoData: -9999.0
• Metadata: Data no filename (regex: .*_(\d{8})\.tif)

GeoServer ImageMosaic:
• Auto-indexação via indexer.properties
• Shapefile index gerado automaticamente
• Time dimension extraída do filename
• WMS GetMap com parâmetro TIME=2025-01-15

---

FORMATO 2: NetCDF HISTÓRICO (Para queries rápidas)

Estrutura de arquivos:
/mnt/workwork/geoserver_data/
├── chirps_hist/historical.nc
├── merge_hist/historical.nc
├── temp_max_hist/historical.nc
├── temp_min_hist/historical.nc
├── temp_hist/historical.nc
├── ndvi_s2_hist/historical.nc
└── ndvi_modis_hist/historical.nc

Estrutura interna (exemplo: chirps_hist/historical.nc):
Dimensions:
  time: UNLIMITED (16000+ registros)
  lat: 810 pontos (-56.0 to 25.0, step 0.1°)
  lon: 600 pontos (-94.0 to -34.0, step 0.1°)

Variables:
  precipitation(time, lat, lon): float32
    units: "mm"
    _FillValue: -9999.0
    chunksizes: [1, 20, 20]
    compression: zlib level 4

  time: datetime64[ns]
    calendar: "proleptic_gregorian"

  lat: float64
  lon: float64

Chunking strategy:
• time=1: Otimizado para queries de séries temporais (1 ponto, muitas datas)
• lat=20, lon=20: Balanceamento entre queries de ponto e área
• Tamanho do chunk: ~1.6 KB (evita chunks muito pequenos)

Vantagens:
• Query de série temporal (1 ponto, 40 anos): ~50ms
• Query de área (bounding box, 1 data): ~200ms
• Dask lazy loading: Só lê chunks necessários
• Metadata centralizado (atributos globais)

---

SINCRONIZAÇÃO

Pattern de atualização:
1. Download raw data (NetCDF temporário)
2. Check missing dates:
   - missing_geotiff: Datas faltando em GeoTIFF mosaics
   - missing_historical: Datas faltando em historical.nc
   - missing_download: União de ambos

3. Processar para GeoTIFF (se missing_geotiff):
   - Reprojeção para EPSG:4326
   - Crop para LatAm bbox
   - Conversão de unidades (K → °C para temperatura)
   - Write individual GeoTIFFs

4. Append para NetCDF histórico (se missing_historical):
   - Open historical.nc em modo append
   - Concatenar nova dimensão time
   - Rechunk e compress
   - Update time bounds

5. Cleanup raw files
6. Trigger GeoServer reindex (se novos GeoTIFFs)

=================================================================================
SLIDE 5 - SHARED DASK CLUSTER E CLIMATE DATA SERVICE
=================================================================================

PROBLEMA DE MEMÓRIA RESOLVIDO

Antes (arquitetura antiga):
• Cada dataset tinha seu próprio Dask client
• 7 datasets × 4 workers × 6GB = 168 GB RAM teórica
• Overhead de múltiplos schedulers
• Competição por recursos

Depois (arquitetura atual):
• 1 shared Dask client para TODOS os datasets
• 4 workers × 6GB = 24 GB RAM total
• 1 scheduler único e eficiente
• ~50% de economia de memória observada

---

CLIMATE DATA SERVICE (app/services/climate_data.py)

Padrão Singleton:
```python
_dask_client = None
_datasets = {
    'precipitation': {},  # {'chirps': xr.Dataset, 'merge': xr.Dataset}
    'temperature': {},    # {'temp_max': xr.Dataset, 'temp_min': xr.Dataset, 'temp': xr.Dataset}
    'ndvi': {}           # {'sentinel2': xr.Dataset, 'modis': xr.Dataset}
}

def get_dask_client():
    """Retorna o shared Dask client (cria se não existir)"""
    global _dask_client
    if _dask_client is None:
        _dask_client = Client(
            n_workers=4,
            threads_per_worker=2,
            memory_limit='6GB',
            dashboard_address=':8787'
        )
    return _dask_client

def load_precipitation_datasets():
    """Carrega CHIRPS e MERGE em memória no startup"""
    client = get_dask_client()
    _datasets['precipitation']['chirps'] = xr.open_dataset(
        f"{DATA_DIR}/chirps_hist/historical.nc",
        chunks={'time': 1, 'lat': 20, 'lon': 20}
    )
    _datasets['precipitation']['merge'] = xr.open_dataset(...)

def get_dataset(data_type: str, source: str) -> xr.Dataset:
    """Acesso unificado a qualquer dataset"""
    return _datasets[data_type][source]
```

Startup sequence (app/api/main.py):
```python
@app.on_event("startup")
async def startup_event():
    logger.info("Initializing shared Dask cluster...")
    client = get_dask_client()

    logger.info("Loading precipitation datasets...")
    load_precipitation_datasets()

    logger.info("Loading temperature datasets...")
    load_temperature_datasets()

    logger.info("Loading NDVI datasets...")
    load_ndvi_datasets()

    logger.info("All datasets loaded successfully!")
```

Vantagens:
• Datasets carregados 1x no startup (não a cada request)
• Lazy loading: Metadata em RAM, data on-disk até .compute()
• Shared scheduler otimiza tasks entre diferentes queries
• Dashboard em localhost:8787 para debug

=================================================================================
SLIDE 6 - PADRÃO DE QUERIES ASSÍNCRONAS
=================================================================================

PROBLEMA: ASYNC/AWAIT vs DASK COMPUTE

FastAPI é assíncrono (asyncio event loop):
• Endpoints são `async def`
• Permite concorrência de múltiplas requisições
• MAS: operações blocking travam o event loop

Dask .compute() é blocking (synchronous):
• Executa computação e espera resultado
• Se chamado direto em async endpoint = trava event loop
• Outras requisições ficam esperando

---

SOLUÇÃO: asyncio.to_thread()

Pattern correto:
```python
# ❌ ERRADO - Bloqueia event loop
@router.post("/history")
async def get_history(request: HistoryRequest):
    ds = get_dataset('precipitation', request.source)
    ts = ds.sel(lat=request.lat, lon=request.lon, method="nearest")
    result = ts.compute()  # BLOQUEIA!
    return result

# ✅ CORRETO - Executa em thread pool
@router.post("/history")
async def get_history(request: HistoryRequest):
    ds = get_dataset('precipitation', request.source)
    result = await asyncio.to_thread(_query_sync, ds, request)
    return result

def _query_sync(ds: xr.Dataset, request: HistoryRequest):
    """Função síncrona executada em thread separada"""
    ts = ds.sel(
        lat=request.lat,
        lon=request.lon,
        method="nearest",
        tolerance=0.1
    )
    # .compute() OK aqui - estamos em thread pool
    return ts.compute()
```

Por que funciona:
• asyncio.to_thread() executa função sync em ThreadPoolExecutor
• Event loop permanece livre para processar outras requisições
• Dask compute roda em thread dedicada
• Resultado é awaited e retornado quando pronto

---

TYPES DE QUERIES IMPLEMENTADAS

1. POINT QUERY (História em 1 coordenada)
```python
ds.sel(lat=-15.8, lon=-47.9, method="nearest", tolerance=0.1)
  .sel(time=slice("2020-01-01", "2025-01-01"))
  .compute()
```
Retorna: Série temporal para 1 ponto

2. AREA QUERY (Estatísticas sobre bounding box)
```python
ds.sel(lat=slice(-16.0, -15.0), lon=slice(-48.0, -47.0))
  .sel(time=slice("2024-01-01", "2024-12-31"))
  .mean(dim=['lat', 'lon'])  # ou .max(), .min()
  .compute()
```
Retorna: Série temporal com média da área

3. POLYGON QUERY (Clip por geometria complexa)
```python
# 1. Criar máscara do polígono
from shapely.geometry import shape
geom = shape(geojson_polygon)

# 2. Criar grid de coordenadas
lats, lons = np.meshgrid(ds.lat.values, ds.lon.values, indexing='ij')
points = [Point(lon, lat) for lat, lon in zip(lats.flatten(), lons.flatten())]

# 3. Aplicar máscara
mask = np.array([geom.contains(p) for p in points]).reshape(lats.shape)

# 4. Query com máscara
ds_masked = ds.where(mask)
result = ds_masked.mean(dim=['lat', 'lon']).compute()
```
Retorna: Estatísticas apenas dentro do polígono

4. TRIGGER QUERY (Datas que ultrapassam threshold)
```python
# Exemplo: Dias com chuva > 50mm
ts = ds.sel(lat=lat, lon=lon, method="nearest").compute()
exceedances = ts.where(ts > 50, drop=True)
trigger_dates = exceedances.time.values
```
Retorna: Lista de datas que atingiram o gatilho

=================================================================================
SLIDE 7 - GEOSERVER INTEGRATION
=================================================================================

GEOSERVER COMO RENDERING ENGINE

Por que usar GeoServer:
• Renderização de raster otimizada (GDAL/JNI)
• WMS padrão OGC (compatível com QGIS, ArcGIS, etc.)
• Time dimension nativa para séries temporais
• Cache automático de tiles (GeoWebCache)
• SLD styling avançado (rampas de cores, classificação)

---

CONFIGURAÇÃO DE IMAGEMOSAIC

Estrutura de diretório:
```
/mnt/workwork/geoserver_data/chirps/
├── chirps_20250101.tif
├── chirps_20250102.tif
├── ...
├── indexer.properties        # Configuração de indexação
├── timeregex.properties       # Regex para extrair data
└── chirps.shp                 # Shapefile index (auto-gerado)
```

indexer.properties:
```properties
TimeAttribute=timestamp
Schema=*the_geom:Polygon,location:String,timestamp:java.util.Date
PropertyCollectors=TimestampFileNameExtractorSPI[timeregex](timestamp)
Caching=false
AbsolutePath=true
```

timeregex.properties:
```properties
regex=.*_(\\d{8})\\.tif
format=yyyyMMdd
```

Como funciona:
1. GeoServer escaneia o diretório
2. Para cada .tif, extrai data do filename via regex
3. Cria registro no shapefile com:
   - the_geom: Bounding box do GeoTIFF
   - location: Path absoluto do arquivo
   - timestamp: Data extraída (java.util.Date)
4. WMS queries com TIME parameter usam o shapefile para encontrar arquivo correto

---

REINDEXAÇÃO APÓS NOVOS DADOS

Métodos:

1. REST API (usado pelos flows):
```bash
curl -u admin:password -X POST \
  "http://localhost:8080/geoserver/rest/workspaces/precipitation_ws/coveragestores/chirps/external.imagemosaic?recalculate=all"
```

2. Programático (Python task):
```python
@task
def refresh_mosaic_shapefile(source: str):
    """Força reindexação do ImageMosaic"""
    url = f"{GEOSERVER_URL}/rest/workspaces/{workspace}/coveragestores/{source}/external.imagemosaic"
    response = requests.post(
        url,
        params={'recalculate': 'all'},
        auth=(GEOSERVER_USER, GEOSERVER_PASSWORD)
    )
    return response.status_code == 200
```

3. Manual (deletar shapefiles):
```bash
rm /path/to/mosaic/*.shp *.dbf *.shx *.prj
# GeoServer recria automaticamente no próximo acesso
```

---

SLD STYLING

Exemplo: Precipitação com rampa de cores

```xml
<ColorMap type="ramp">
  <ColorMapEntry color="#FFFFFF" quantity="0" label="0 mm" opacity="0"/>
  <ColorMapEntry color="#E0F3FF" quantity="1" label="1 mm"/>
  <ColorMapEntry color="#73B2FF" quantity="10" label="10 mm"/>
  <ColorMapEntry color="#0059FF" quantity="25" label="25 mm"/>
  <ColorMapEntry color="#00C800" quantity="50" label="50 mm"/>
  <ColorMapEntry color="#FFFF00" quantity="75" label="75 mm"/>
  <ColorMapEntry color="#FF9000" quantity="100" label="100 mm"/>
  <ColorMapEntry color="#FF0000" quantity="150" label="150 mm"/>
  <ColorMapEntry color="#8B0000" quantity="200" label="200+ mm"/>
</ColorMap>
```

Styles diferentes por variável:
• Precipitação: Azul → Verde → Amarelo → Vermelho
• Temperatura: Azul (frio) → Branco → Vermelho (quente)
• NDVI: Marrom (solo) → Verde claro → Verde escuro (vegetação densa)
• Raios: Amarelo → Laranja → Vermelho (densidade crescente)

---

WMS PROXY NA API

Problema: Expor GeoServer URL diretamente ao cliente
Solução: FastAPI proxy endpoint

```python
@router.get("/wms")
async def proxy_wms(
    layers: str,
    bbox: str,
    width: int,
    height: int,
    time: str,
    format: str = "image/png"
):
    """Proxy WMS request para GeoServer"""
    params = {
        'service': 'WMS',
        'version': '1.1.1',
        'request': 'GetMap',
        'layers': layers,
        'bbox': bbox,
        'width': width,
        'height': height,
        'srs': 'EPSG:4326',
        'format': format,
        'time': time
    }

    async with httpx.AsyncClient() as client:
        response = await client.get(GEOSERVER_WMS_URL, params=params)
        return Response(content=response.content, media_type=format)
```

Vantagens:
• Cliente não precisa saber URL do GeoServer
• Possibilidade de adicionar auth/rate limiting
• Logs centralizados de uso de mapas

=================================================================================
SLIDE 8 - PREFECT FLOWS E DATA PIPELINES
=================================================================================

PREFECT COMO ORCHESTRATOR

Por que Prefect:
• Retry automático de tasks com falha
• Paralelização nativa (concurrent.futures, Dask)
• Logging estruturado por task
• Visualização de DAG (flow execution graph)
• Scheduling (via Prefect Cloud ou cron)

---

ESTRUTURA DE UM FLOW TÍPICO

Exemplo: ERA5 Temperature Flow

```python
@flow(name="ERA5 Daily Temperature")
def era5_daily_temperature_flow(
    start_date: str,
    end_date: str,
    variable: str = "temp_max"
):
    """
    Processa dados de temperatura ERA5-Land

    Steps:
    1. Check missing dates (GeoTIFF + historical.nc)
    2. Download only missing data from CDS API
    3. Process to GeoTIFF (if missing from mosaics)
    4. Append to historical NetCDF (if missing from historical)
    5. Cleanup raw files
    6. Refresh GeoServer index
    """

    # Task 1: Check what's missing
    missing_info = check_missing_dates(start_date, end_date, variable)

    if not missing_info['download']:
        logger.info("No missing data, skipping download")
        return

    # Task 2: Download from CDS API
    batch_path = download_era5_land_daily_batch(
        missing_info['download'],
        variable
    )

    # Task 3: Process to GeoTIFF (parallel)
    if missing_info['geotiff']:
        process_era5_land_daily_to_geotiff(
            batch_path,
            dates_to_process=missing_info['geotiff']
        )

    # Task 4: Append to historical NetCDF
    if missing_info['historical']:
        append_to_historical_netcdf(
            batch_path,
            variable,
            dates_to_append=missing_info['historical']
        )

    # Task 5: Cleanup
    cleanup_raw_files(batch_path)

    # Task 6: Reindex GeoServer (if new GeoTIFFs)
    if missing_info['geotiff']:
        refresh_mosaic_shapefile(variable)
```

---

TASKS COM RETRY E TIMEOUT

```python
@task(
    retries=3,
    retry_delay_seconds=60,
    timeout_seconds=3600
)
def download_era5_land_daily_batch(dates: List[str], variable: str):
    """
    Download ERA5-Land data via CDS API

    Retries: 3x com 60s de delay (CDS API pode ter rate limiting)
    Timeout: 1h (downloads grandes podem demorar)
    """
    import cdsapi

    c = cdsapi.Client()

    request = {
        'variable': variable,
        'year': [d.split('-')[0] for d in dates],
        'month': [d.split('-')[1] for d in dates],
        'day': [d.split('-')[2] for d in dates],
        'time': '00:00',
        'area': latam_bbox_cds,  # [N, W, S, E]
        'format': 'netcdf'
    }

    output_path = f"{RAW_DIR}/era5_land_daily/{variable}_{dates[0]}_{dates[-1]}.nc"

    c.retrieve('reanalysis-era5-land', request, output_path)

    return output_path
```

---

PARALELIZAÇÃO COM DASK

Processing de múltiplos GeoTIFFs em paralelo:

```python
@task
def process_era5_land_daily_to_geotiff(batch_path: str, dates_to_process: List[str]):
    """Converte NetCDF para GeoTIFFs individuais (paralelo)"""

    ds = xr.open_dataset(batch_path)

    # Converter K → °C
    if 'temperature' in ds.data_vars:
        ds['temperature'] = ds['temperature'] - 273.15

    # Função para processar 1 data
    def process_single_date(date_str: str):
        ds_day = ds.sel(time=date_str)

        output_path = f"{DATA_DIR}/{variable}/{variable}_{date_str.replace('-', '')}.tif"

        # Write GeoTIFF com rasterio
        with rasterio.open(
            output_path,
            'w',
            driver='GTiff',
            height=ds_day.lat.size,
            width=ds_day.lon.size,
            count=1,
            dtype='float32',
            crs='EPSG:4326',
            transform=transform,
            compress='lzw',
            nodata=-9999
        ) as dst:
            dst.write(ds_day['temperature'].values, 1)

    # Executar em paralelo via Dask
    from dask import delayed, compute
    tasks = [delayed(process_single_date)(d) for d in dates_to_process]
    compute(*tasks, scheduler='threads', num_workers=4)
```

---

CHECK MISSING DATES (Incremental Updates)

```python
@task
def check_missing_dates(start_date: str, end_date: str, variable: str):
    """
    Verifica quais datas faltam em GeoTIFF e historical.nc
    Retorna apenas o que realmente precisa ser baixado/processado
    """

    all_dates = pd.date_range(start_date, end_date, freq='D')

    # Check GeoTIFF files
    geotiff_dir = Path(f"{DATA_DIR}/{variable}")
    existing_geotiffs = set()
    for f in geotiff_dir.glob(f"{variable}_*.tif"):
        date_str = f.stem.split('_')[1]  # chirps_20250101.tif → 20250101
        existing_geotiffs.add(pd.to_datetime(date_str, format='%Y%m%d'))

    missing_geotiff = [d for d in all_dates if d not in existing_geotiffs]

    # Check historical.nc
    hist_path = f"{DATA_DIR}/{variable}_hist/historical.nc"
    if Path(hist_path).exists():
        ds_hist = xr.open_dataset(hist_path)
        existing_historical = set(pd.to_datetime(ds_hist.time.values))
        missing_historical = [d for d in all_dates if d not in existing_historical]
    else:
        missing_historical = list(all_dates)

    # Missing download = union of both
    missing_download = sorted(set(missing_geotiff) | set(missing_historical))

    return {
        'geotiff': missing_geotiff,
        'historical': missing_historical,
        'download': missing_download
    }
```

Isso evita:
• Re-download de dados já existentes
• Re-processamento de GeoTIFFs já criados
• Duplicação de datas no historical.nc

=================================================================================
SLIDE 9 - PROCESSAMENTO DE RAIOS (GLM)
=================================================================================

DESAFIO ESPECÍFICO: GLM

Diferente dos outros datasets:
• Dados em arquivos separados (1 arquivo a cada 20 segundos)
• ~4300 arquivos por dia
• Formato NetCDF Level 2 (não gridded, eventos pontuais)
• Precisa agregação espacial e temporal

---

PIPELINE GLM

1. DOWNLOAD (S3 público NOAA)
```python
@task(retries=3, retry_delay_seconds=30)
def download_glm_files_for_date(date: str):
    """Download todos os arquivos GLM de 1 dia do S3"""

    bucket = 'noaa-goes16'
    prefix = f'GLM-L2-LCFA/{date.year}/{date.dayofyear:03d}/'

    import boto3
    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))

    files = []
    paginator = s3.get_paginator('list_objects_v2')
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get('Contents', []):
            if obj['Key'].endswith('.nc'):
                files.append(obj['Key'])

    # Download paralelo
    from concurrent.futures import ThreadPoolExecutor
    def download_file(key):
        local_path = f"{RAW_DIR}/glm/{Path(key).name}"
        s3.download_file(bucket, key, local_path)
        return local_path

    with ThreadPoolExecutor(max_workers=10) as executor:
        local_files = list(executor.map(download_file, files))

    return local_files
```

2. AGREGAÇÃO EM GRID REGULAR (30 minutos)
```python
@task
def aggregate_glm_to_grid(nc_files: List[str], date: str):
    """
    Agrega eventos pontuais GLM em grid regular 0.1°
    Acumulação: 30 minutos
    Métrica: Flash extent density (flashes/km²/30min)
    """

    # Criar grid de saída
    lats = np.arange(25, -56, -0.1)  # LatAm bbox
    lons = np.arange(-94, -34, 0.1)
    grid = np.zeros((len(lats), len(lons), 48))  # 48 períodos de 30min

    for nc_file in nc_files:
        ds = xr.open_dataset(nc_file)

        # Extrair coordenadas dos flashes
        flash_lats = ds['flash_lat'].values
        flash_lons = ds['flash_lon'].values
        flash_times = ds['flash_time_offset_of_first_event'].values

        # Binning espacial e temporal
        lat_idx = ((25 - flash_lats) / 0.1).astype(int)
        lon_idx = ((flash_lons + 94) / 0.1).astype(int)
        time_idx = (flash_times / 1800).astype(int)  # 1800s = 30min

        # Acumular no grid
        for i in range(len(flash_lats)):
            if (0 <= lat_idx[i] < len(lats) and
                0 <= lon_idx[i] < len(lons) and
                0 <= time_idx[i] < 48):
                grid[lat_idx[i], lon_idx[i], time_idx[i]] += 1

    # Converter para flash density (flashes/km²)
    # Área de cada célula 0.1° × 0.1° ≈ 123 km² no equador
    area_km2 = 123
    grid = grid / area_km2

    # Criar xarray Dataset
    ds_gridded = xr.Dataset(
        {
            'flash_density': (['lat', 'lon', 'time_slot'], grid)
        },
        coords={
            'lat': lats,
            'lon': lons,
            'time_slot': range(48)
        }
    )

    # Salvar NetCDF intermediário
    output_path = f"{RAW_DIR}/glm_gridded/glm_{date}.nc"
    ds_gridded.to_netcdf(output_path)

    return output_path
```

3. DAILY MAXIMUM (Para GeoTIFF)
```python
@task
def create_daily_glm_geotiff(gridded_nc: str, date: str):
    """
    Cria GeoTIFF diário com MÁXIMO dos 48 períodos de 30min
    Representa a intensidade máxima de raios no dia
    """

    ds = xr.open_dataset(gridded_nc)

    # Máximo diário através dos time slots
    daily_max = ds['flash_density'].max(dim='time_slot')

    # Write GeoTIFF
    output_path = f"{DATA_DIR}/glm_fed/glm_fed_{date.replace('-', '')}.tif"

    with rasterio.open(
        output_path,
        'w',
        driver='GTiff',
        height=len(ds.lat),
        width=len(ds.lon),
        count=1,
        dtype='float32',
        crs='EPSG:4326',
        transform=rasterio.transform.from_bounds(
            -94, -56, -34, 25,
            len(ds.lon), len(ds.lat)
        ),
        compress='lzw',
        nodata=-9999
    ) as dst:
        dst.write(daily_max.values, 1)
```

---

OTIMIZAÇÕES GLM

• Download paralelo (10 threads)
• Processamento em batches (100 arquivos por vez) para não estourar RAM
• Numba JIT compilation para binning espacial (100x mais rápido)
• Chunked write para historical.nc (não carregar tudo em memória)

=================================================================================
SLIDE 10 - NDVI VIA MICROSOFT PLANETARY COMPUTER
=================================================================================

PLANETARY COMPUTER STAC API

STAC (SpatioTemporal Asset Catalog):
• Padrão aberto para descoberta de dados geoespaciais
• JSON com metadados + links para assets (GeoTIFFs)
• Query por bbox, datetime, collection, propriedades

Microsoft Planetary Computer:
• Catálogo público de dados de Earth Observation
• Petabytes de Sentinel, Landsat, MODIS, etc.
• 100% gratuito, sem necessidade de autenticação
• Assets hospedados no Azure (URLs assinadas para acesso)

---

QUERY STAC

```python
import pystac_client
import planetary_computer

# Conectar ao catalog
catalog = pystac_client.Client.open(
    "https://planetarycomputer.microsoft.com/api/stac/v1"
)

# Buscar Sentinel-2 scenes
bbox = [-48.0, -16.0, -47.0, -15.0]  # LatAm region
date_range = "2025-01-01/2025-01-31"

search = catalog.search(
    collections=["sentinel-2-l2a"],
    bbox=bbox,
    datetime=date_range,
    query={"eo:cloud_cover": {"lt": 20}}  # Menos de 20% nuvens
)

items = list(search.items())
print(f"Found {len(items)} scenes")
```

---

PROCESSAR SENTINEL-2 PARA NDVI

```python
@task
def process_sentinel2_ndvi(item: pystac.Item, date: str):
    """
    Calcula NDVI de cena Sentinel-2
    NDVI = (NIR - Red) / (NIR + Red)
    """

    # Assinar URLs (necessário para acesso)
    signed_item = planetary_computer.sign(item)

    # Ler bandas Red (B04) e NIR (B08)
    red_href = signed_item.assets['B04'].href
    nir_href = signed_item.assets['B08'].href

    with rasterio.open(red_href) as red_src:
        red = red_src.read(1).astype(float)
        profile = red_src.profile

    with rasterio.open(nir_href) as nir_src:
        nir = nir_src.read(1).astype(float)

    # Calcular NDVI
    ndvi = (nir - red) / (nir + red + 1e-8)  # +epsilon para evitar div/0

    # Aplicar máscara de nuvens (SCL band)
    scl_href = signed_item.assets['SCL'].href
    with rasterio.open(scl_href) as scl_src:
        scl = scl_src.read(1)
        # SCL: 3=cloud shadow, 8=cloud medium, 9=cloud high
        cloud_mask = (scl == 3) | (scl == 8) | (scl == 9)
        ndvi[cloud_mask] = -9999  # NoData

    # Reprojetar para EPSG:4326 e crop para LatAm bbox
    with rasterio.open('memory', 'w+', **profile) as mem:
        mem.write(ndvi, 1)

        # Warp para WGS84
        with rasterio.vrt.WarpedVRT(
            mem,
            crs='EPSG:4326',
            resampling=Resampling.bilinear
        ) as vrt:
            # Crop para bbox
            window = vrt.window(*latam_bbox_raster)
            ndvi_cropped = vrt.read(1, window=window)

    # Write GeoTIFF final
    output_path = f"{DATA_DIR}/ndvi_s2/ndvi_s2_{date.replace('-', '')}.tif"
    # ... write com rasterio
```

---

MOSAICO DE MÚLTIPLAS CENAS

Sentinel-2 tiles são pequenos (~100km × 100km):
• LatAm completo precisa ~500 tiles
• Mesma data pode ter múltiplas cenas

Solução: Mosaico com máximo NDVI (composite)

```python
@task
def create_daily_ndvi_mosaic(date: str):
    """
    Cria mosaico diário de todas as cenas Sentinel-2 disponíveis
    Composite: máximo NDVI (remove nuvens residuais)
    """

    # Buscar todas as cenas do dia
    items = search_sentinel2_for_date(date, latam_bbox)

    # Processar cada cena para NDVI
    ndvi_files = []
    for item in items:
        ndvi_path = process_sentinel2_ndvi(item, date)
        ndvi_files.append(ndvi_path)

    # Mosaico com rasterio.merge (método: max)
    from rasterio.merge import merge

    src_files = [rasterio.open(f) for f in ndvi_files]
    mosaic, out_transform = merge(
        src_files,
        method='max',  # Máximo NDVI (vegetação mais saudável)
        nodata=-9999
    )

    # Write mosaic final
    output_path = f"{DATA_DIR}/ndvi_s2/ndvi_s2_{date.replace('-', '')}.tif"
    # ... write
```

---

MODIS (Mais simples, já é produto)

```python
@task
def download_modis_ndvi(date: str):
    """
    MODIS MOD13Q1 já vem com NDVI calculado
    Produto: Composição 16 dias
    """

    search = catalog.search(
        collections=["modis-13Q1-061"],
        bbox=latam_bbox,
        datetime=date
    )

    item = next(search.items())
    signed_item = planetary_computer.sign(item)

    # Asset '250m_16_days_NDVI' já é NDVI pronto
    ndvi_href = signed_item.assets['250m_16_days_NDVI'].href

    # Download direto
    with rasterio.open(ndvi_href) as src:
        ndvi = src.read(1)
        # Escala: valores vêm em range -2000 a 10000
        # Converter para -1.0 a 1.0
        ndvi = ndvi / 10000.0

        # Reprojetar e crop...
```

=================================================================================
SLIDE 11 - API ENDPOINTS E SCHEMAS
=================================================================================

ESTRUTURA DE ROUTERS

```
app/api/routers/
├── precipitation.py    # CHIRPS + MERGE endpoints
├── temperature.py      # ERA5 temp_max, temp_min, temp endpoints
├── lightning.py        # GLM endpoints
├── wind.py            # ERA5 wind endpoints
└── ndvi.py            # Sentinel-2 + MODIS endpoints
```

Cada router tem endpoints padronizados:
• POST /history - Série temporal de 1 ponto
• POST /triggers - Datas que ultrapassam threshold
• POST /triggers/area - Triggers para área (bbox ou polígono)
• GET /wms - Proxy para GeoServer (mapa visual)

---

SCHEMAS (Pydantic)

```python
# app/api/schemas/precipitation.py

class HistoryRequest(BaseModel):
    """Request para série temporal de precipitação"""

    lat: float = Field(..., ge=-56, le=25, description="Latitude (WGS84)")
    lon: float = Field(..., ge=-94, le=-34, description="Longitude (WGS84)")
    start_date: str = Field(..., description="Data inicial (YYYY-MM-DD)")
    end_date: str = Field(..., description="Data final (YYYY-MM-DD)")
    source: Literal["chirps", "merge"] = Field(default="chirps")

    @validator('start_date', 'end_date')
    def validate_date_format(cls, v):
        try:
            pd.to_datetime(v)
        except:
            raise ValueError("Data deve estar no formato YYYY-MM-DD")
        return v

class HistoryResponse(BaseModel):
    """Response com série temporal"""

    lat: float
    lon: float
    source: str
    data: List[Dict[str, Any]]  # [{"date": "2025-01-01", "value": 12.5}, ...]
    statistics: Dict[str, float]  # {"mean": 5.2, "max": 120.0, ...}

class TriggerRequest(BaseModel):
    """Request para detecção de gatilhos"""

    lat: float
    lon: float
    start_date: str
    end_date: str
    threshold: float = Field(..., description="Valor de gatilho (mm para precipitação)")
    condition: Literal["greater", "less", "equal"] = "greater"
    source: Literal["chirps", "merge"] = "chirps"

class TriggerResponse(BaseModel):
    """Response com datas que atingiram gatilho"""

    lat: float
    lon: float
    threshold: float
    condition: str
    exceedances: List[Dict[str, Any]]  # [{"date": "2025-01-15", "value": 85.2}, ...]
    count: int
```

Validações automáticas:
• Tipos de dados (float, str, int)
• Ranges (lat entre -56 e 25)
• Formatos (YYYY-MM-DD para datas)
• Enums (source só pode ser "chirps" ou "merge")
• Campos obrigatórios vs. opcionais

---

ENDPOINT EXAMPLE: HISTORY

```python
@router.post("/history", response_model=HistoryResponse)
async def get_precipitation_history(request: HistoryRequest):
    """
    Retorna série temporal de precipitação para um ponto

    - Usa historical.nc via climate_data service
    - Executa em thread pool (não bloqueia event loop)
    - Retorna JSON com dados + estatísticas
    """

    try:
        # Get dataset (loaded on startup)
        ds = get_dataset('precipitation', request.source)

        # Execute query em thread pool
        result = await asyncio.to_thread(
            _query_precipitation_history,
            ds,
            request
        )

        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def _query_precipitation_history(ds: xr.Dataset, request: HistoryRequest):
    """Sync function executed in thread pool"""

    # Select point with nearest neighbor
    ts = ds.sel(
        lat=request.lat,
        lon=request.lon,
        method="nearest",
        tolerance=0.1  # Max 0.1° distance
    )

    # Select time range
    ts = ts.sel(time=slice(request.start_date, request.end_date))

    # Compute (blocking, but OK in thread)
    ts_computed = ts.compute()

    # Format response
    data = [
        {
            "date": str(t.values)[:10],
            "value": float(ts_computed['precipitation'].sel(time=t).values)
        }
        for t in ts_computed.time
    ]

    # Calculate statistics
    values = ts_computed['precipitation'].values
    statistics = {
        "mean": float(np.mean(values)),
        "median": float(np.median(values)),
        "std": float(np.std(values)),
        "min": float(np.min(values)),
        "max": float(np.max(values)),
        "count": int(len(values))
    }

    return HistoryResponse(
        lat=request.lat,
        lon=request.lon,
        source=request.source,
        data=data,
        statistics=statistics
    )
```

---

ENDPOINT EXAMPLE: AREA TRIGGERS

```python
@router.post("/triggers/area", response_model=AreaTriggerResponse)
async def get_area_triggers(request: AreaTriggerRequest):
    """
    Detecta gatilhos em área (bbox ou polígono)

    Retorna:
    - Estatísticas espaciais por data (mean, max, min)
    - Datas onde a MÉDIA da área ultrapassou threshold
    - Mapa de % da área que ultrapassou threshold
    """

    ds = get_dataset('precipitation', request.source)

    result = await asyncio.to_thread(
        _calculate_area_exceedances,
        ds,
        request
    )

    return result

def _calculate_area_exceedances(ds: xr.Dataset, request: AreaTriggerRequest):
    """Calculate area-based triggers"""

    # Spatial selection
    if request.geometry_type == "bbox":
        ds_area = ds.sel(
            lat=slice(request.lat_max, request.lat_min),
            lon=slice(request.lon_min, request.lon_max)
        )
    elif request.geometry_type == "polygon":
        # Apply polygon mask
        mask = create_polygon_mask(ds, request.polygon)
        ds_area = ds.where(mask)

    # Temporal selection
    ds_area = ds_area.sel(time=slice(request.start_date, request.end_date))

    # Compute spatial statistics per date
    mean_per_date = ds_area.mean(dim=['lat', 'lon']).compute()
    max_per_date = ds_area.max(dim=['lat', 'lon']).compute()

    # Find dates where MEAN exceeded threshold
    exceedances = mean_per_date.where(
        mean_per_date['precipitation'] > request.threshold,
        drop=True
    )

    # Calculate % of area exceeding threshold per date
    def pct_area_exceeding(date):
        day_data = ds_area.sel(time=date)['precipitation'].values
        valid_pixels = ~np.isnan(day_data)
        if valid_pixels.sum() == 0:
            return 0.0
        exceeding = day_data > request.threshold
        return (exceeding.sum() / valid_pixels.sum()) * 100

    # Format response
    exceedance_list = []
    for t in exceedances.time:
        exceedance_list.append({
            "date": str(t.values)[:10],
            "mean_value": float(mean_per_date.sel(time=t)['precipitation'].values),
            "max_value": float(max_per_date.sel(time=t)['precipitation'].values),
            "pct_area_exceeding": pct_area_exceeding(t)
        })

    return AreaTriggerResponse(
        threshold=request.threshold,
        exceedances=exceedance_list,
        count=len(exceedance_list)
    )
```

=================================================================================
SLIDE 12 - MONITORING E DEBUGGING
=================================================================================

DASK DASHBOARD

URL: http://localhost:8787

Abas principais:
• **Status**: Workers ativos, memória, CPU
• **Task Stream**: Timeline de tasks executadas (útil para identificar gargalos)
• **Progress**: Tasks em execução, fila, completadas
• **Graph**: DAG de dependências de tasks
• **Memory**: Uso de memória por worker (identifica memory leaks)

Como usar durante debug:
1. Abrir dashboard em navegador
2. Fazer query na API
3. Observar Task Stream:
   - Cores: Verde (sucesso), Vermelho (erro), Amarelo (em execução)
   - Duração: Identifica operações lentas
   - Paralelismo: Ver se tasks estão executando em paralelo ou sequencialmente

---

LOGGING

FastAPI logging (app/api/main.py):
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/var/log/fastapi/api.log')
    ]
)

logger = logging.getLogger(__name__)

@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Log todas as requisições"""
    start_time = time.time()

    response = await call_next(request)

    duration = time.time() - start_time
    logger.info(
        f"{request.method} {request.url.path} "
        f"status={response.status_code} duration={duration:.2f}s"
    )

    return response
```

Prefect logging (flows):
```python
from prefect import get_run_logger

@flow
def my_flow():
    logger = get_run_logger()
    logger.info("Flow started")
    logger.warning("Missing 5 dates, will download")
    logger.error("Download failed, retrying...")
```

---

HEALTH CHECKS

```python
@app.get("/health")
async def health_check():
    """Basic health check"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/status")
async def detailed_status():
    """Detailed status with diagnostics"""

    # Check Dask cluster
    dask_client = get_dask_client()
    dask_status = {
        "workers": len(dask_client.scheduler_info()['workers']),
        "memory_total": sum(w['memory_limit'] for w in dask_client.scheduler_info()['workers'].values()),
        "tasks_processing": len(dask_client.processing()),
    }

    # Check datasets loaded
    datasets_status = {
        'precipitation': list(get_loaded_datasets('precipitation').keys()),
        'temperature': list(get_loaded_datasets('temperature').keys()),
        'ndvi': list(get_loaded_datasets('ndvi').keys())
    }

    # Check GeoServer
    try:
        response = requests.get(f"{GEOSERVER_URL}/rest/about/version.json", auth=(GEOSERVER_USER, GEOSERVER_PASSWORD), timeout=5)
        geoserver_status = "healthy" if response.status_code == 200 else "unreachable"
    except:
        geoserver_status = "unreachable"

    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "dask": dask_status,
        "datasets": datasets_status,
        "geoserver": geoserver_status
    }
```

---

DEBUGGING QUERIES

Técnicas úteis:

1. Verificar shape dos arrays antes de .compute():
```python
ts = ds.sel(lat=lat, lon=lon, method="nearest")
print(f"Shape antes de compute: {ts.shape}")
print(f"Chunks: {ts.chunks}")
# Só compute se shape faz sentido
result = ts.compute()
```

2. Sample pequeno antes de query grande:
```python
# Testar com 1 semana antes de rodar 40 anos
ts_sample = ds.sel(time=slice("2025-01-01", "2025-01-07"))
result_sample = ts_sample.compute()  # Rápido
# Se OK, rodar completo
```

3. Usar .load() vs .compute():
```python
# .compute() retorna numpy array (não é mais xarray)
arr = ds.compute()  # Perde metadados

# .load() carrega em memória mas mantém xarray Dataset
ds_loaded = ds.load()  # Mantém coordenadas, atributos
```

4. Profiling com %%time (Jupyter):
```python
%%time
result = ds.sel(lat=-15.8, lon=-47.9).compute()
# Output: CPU times: user 1.2s, sys: 0.5s, total: 1.7s
```

=================================================================================
SLIDE 13 - PERFORMANCE E OTIMIZAÇÕES
=================================================================================

BOTTLENECKS COMUNS E SOLUÇÕES

1. CHUNKING INADEQUADO

Problema:
• Chunks muito pequenos: Overhead de scheduling
• Chunks muito grandes: Não paraleliza, estoura RAM

Solução:
• Target: 100-500 MB por chunk
• Para séries temporais (point queries): time=1
• Para queries espaciais (mapas): time=10-50, lat=100, lon=100

Exemplo:
```python
# ❌ Ruim para point queries
ds = xr.open_dataset('data.nc', chunks={'time': 365, 'lat': 20, 'lon': 20})
# Query de 1 ponto precisa ler 365 dias de 1x

# ✅ Bom para point queries
ds = xr.open_dataset('data.nc', chunks={'time': 1, 'lat': 20, 'lon': 20})
# Query de 1 ponto lê apenas 1 dia de cada vez
```

---

2. MASKS APLICADAS TARDE

Problema:
```python
# ❌ Carrega tudo em memória antes de filtrar
ds_full = ds.compute()
ds_filtered = ds_full.where(ds_full > 10)
```

Solução:
```python
# ✅ Aplica mask antes de compute (lazy)
ds_filtered = ds.where(ds > 10)
result = ds_filtered.compute()  # Só computa pixels necessários
```

---

3. MULTIPLE COMPUTES SEQUENCIAIS

Problema:
```python
# ❌ 3 passes sobre os dados
mean = ds.mean().compute()
max_val = ds.max().compute()
min_val = ds.min().compute()
```

Solução:
```python
# ✅ 1 pass sobre os dados
import dask
mean, max_val, min_val = dask.compute(
    ds.mean(),
    ds.max(),
    ds.min()
)
```

---

4. FALTA DE CACHE

Problema:
• Mesma query repetida múltiplas vezes
• Re-lê arquivo do disco a cada vez

Solução:
```python
# Cache em memória (para datasets pequenos/médios)
ds_cached = ds.load()  # Carrega tudo em RAM
# Queries subsequentes são instantâneas

# Ou LRU cache no Python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_timeseries_cached(lat, lon, start, end):
    return get_timeseries(lat, lon, start, end)
```

---

5. I/O SEQUENCIAL

Problema:
```python
# ❌ Lê 1 arquivo por vez
for date in dates:
    ds = xr.open_dataset(f"data_{date}.nc")
    process(ds)
```

Solução:
```python
# ✅ Abre todos com open_mfdataset (paralelo)
ds = xr.open_mfdataset(
    "data_*.nc",
    parallel=True,
    chunks={'time': 1}
)
# Dask paraleliza leitura de múltiplos arquivos
```

---

OTIMIZAÇÕES IMPLEMENTADAS NA PLATAFORMA

1. **Shared Dask Client**: 1 cluster para todos os datasets (50% menos RAM)

2. **Lazy Loading**: Datasets carregam metadata on startup, data on-demand

3. **Strategic Chunking**: time=1 para queries temporais otimizadas

4. **Asyncio.to_thread**: Queries não bloqueiam event loop do FastAPI

5. **GeoServer Cache**: Tiles WMS cacheados (GeoWebCache)

6. **Compression**: zlib level 4 (balanceamento speed/size)

7. **NoData Masking**: Arrays masked antes de compute

8. **Parallel Processing**: Dask paralleliza automaticamente operações em chunks

=================================================================================
SLIDE 14 - EXPANSÃO E ROADMAP TÉCNICO
=================================================================================

PRÓXIMAS VARIÁVEIS A IMPLEMENTAR

1. RADIAÇÃO SOLAR (Em progresso)

   Fonte: ERA5 hourly ssrd (surface solar radiation downwards)
   - Resolução: 0.25° hourly
   - Agregação: Soma diária (J/m² → kWh/m²/day)
   - Use case: Projetos de energia solar, produtividade de culturas

   Desafio técnico:
   - Conversão de unidades: J/m² acumulado → kWh/m²/day
   - Hourly → Daily aggregation (soma de 24 horas)
   - Correção de albedo para superfícies específicas

2. UMIDADE DO SOLO

   Fonte: ERA5-Land volumetric soil water layer 1-4
   - 4 camadas: 0-7cm, 7-28cm, 28-100cm, 100-289cm
   - Importante para: Modelagem hidrológica, previsão de secas agrícolas

   Desafio técnico:
   - Múltiplas camadas (4 variáveis)
   - Integração com precipitação para análise combinada

3. EVAPOTRANSPIRAÇÃO

   Fonte: ERA5-Land potential evaporation
   - Cálculo: PET (Penman-Monteith)
   - Importante para: Balanço hídrico, necessidade de irrigação

   Derived variable:
   - Déficit hídrico = Precipitação - Evapotranspiração

4. UMIDADE RELATIVA E PONTO DE ORVALHO

   Fonte: ERA5 2m dewpoint temperature
   - Cálculo de RH a partir de temp + dewpoint
   - Importante para: Risco de doenças em culturas, conforto térmico

5. ÍNDICES DERIVADOS

   SPI (Standardized Precipitation Index):
   ```python
   def calculate_spi(precip_ts, window=90):
       """
       SPI via ajuste de distribuição Gamma
       window: janela de acumulação (30, 60, 90, 180 dias)
       """
       from scipy.stats import gamma

       # Acumular precipitação na janela
       precip_cumsum = precip_ts.rolling(time=window).sum()

       # Fit distribuição Gamma aos dados históricos
       shape, loc, scale = gamma.fit(precip_cumsum.values)

       # Calcular CDF (probabilidade cumulativa)
       cdf = gamma.cdf(precip_cumsum, shape, loc, scale)

       # Inverter para distribuição normal padrão
       from scipy.stats import norm
       spi = norm.ppf(cdf)

       return spi
   ```

   Interpretação:
   - SPI > 2.0: Extremamente úmido
   - SPI 1.5 a 2.0: Muito úmido
   - SPI -1.5 a 1.5: Normal
   - SPI -2.0 a -1.5: Seca moderada
   - SPI < -2.0: Seca severa/extrema

   SPEI (Standardized Precipitation Evapotranspiration Index):
   - Similar ao SPI mas inclui evapotranspiração
   - Melhor para cenários de mudanças climáticas

   Anomalias padronizadas:
   ```python
   def calculate_anomaly(ts):
       """Desvio padronizado vs. climatologia"""
       # Climatologia (média por dia do ano, 1981-2010)
       climatology = ts.sel(time=slice('1981', '2010')).groupby('time.dayofyear').mean()

       # Desvio padrão por dia do ano
       std = ts.sel(time=slice('1981', '2010')).groupby('time.dayofyear').std()

       # Anomalia padronizada
       anomaly = (ts.groupby('time.dayofyear') - climatology) / std

       return anomaly
   ```

---

EXPANSÕES DE ARQUITETURA

1. CACHING LAYER (Redis)

   Para queries frequentes:
   ```python
   import redis
   r = redis.Redis()

   @app.get("/history")
   async def get_history(lat, lon, start, end):
       # Check cache
       cache_key = f"history:{lat}:{lon}:{start}:{end}"
       cached = r.get(cache_key)
       if cached:
           return json.loads(cached)

       # Query
       result = await query_history(lat, lon, start, end)

       # Store in cache (TTL 1 hora)
       r.setex(cache_key, 3600, json.dumps(result))

       return result
   ```

2. TASK QUEUE (Celery)

   Para processamento assíncrono pesado:
   ```python
   from celery import Celery

   celery = Celery('tasks', broker='redis://localhost:6379')

   @celery.task
   def process_large_polygon(geojson, variable, start, end):
       """
       Processar polígono complexo em background
       Cliente recebe task_id e faz polling
       """
       result = calculate_polygon_statistics(geojson, variable, start, end)
       return result

   @app.post("/polygon/async")
   async def submit_polygon_task(request: PolygonRequest):
       task = process_large_polygon.delay(request.geojson, ...)
       return {"task_id": task.id, "status": "processing"}

   @app.get("/polygon/status/{task_id}")
   async def check_task_status(task_id: str):
       task = celery.AsyncResult(task_id)
       if task.ready():
           return {"status": "completed", "result": task.result}
       else:
           return {"status": "processing"}
   ```

3. POSTGRESQL + PostGIS

   Para metadata e queries espaciais rápidas:
   ```sql
   CREATE TABLE climate_anomalies (
       id SERIAL PRIMARY KEY,
       date DATE,
       variable VARCHAR(50),
       geom GEOMETRY(Point, 4326),
       anomaly_value FLOAT,
       percentile FLOAT
   );

   CREATE INDEX idx_geom ON climate_anomalies USING GIST(geom);
   CREATE INDEX idx_date ON climate_anomalies(date);

   -- Query: Pontos com anomalia extrema em região
   SELECT * FROM climate_anomalies
   WHERE ST_Within(geom, ST_MakeEnvelope(-48, -16, -47, -15, 4326))
   AND date = '2025-01-15'
   AND anomaly_value > 2.0;
   ```

4. WEBHOOK NOTIFICATIONS

   Alertas automáticos quando threshold é ultrapassado:
   ```python
   class AlertRule(BaseModel):
       name: str
       geometry: GeoJSON
       variable: str
       threshold: float
       condition: Literal["greater", "less"]
       webhook_url: str

   # Cron job diário
   @task
   def check_alert_rules():
       for rule in get_active_rules():
           # Query latest data
           value = query_latest_value(rule.geometry, rule.variable)

           # Check condition
           if (rule.condition == "greater" and value > rule.threshold) or \
              (rule.condition == "less" and value < rule.threshold):
               # Trigger webhook
               requests.post(rule.webhook_url, json={
                   "rule": rule.name,
                   "variable": rule.variable,
                   "value": value,
                   "threshold": rule.threshold,
                   "timestamp": datetime.now().isoformat()
               })
   ```

=================================================================================
SLIDE 15 - CONCLUSÃO TÉCNICA
=================================================================================

STACK TECNOLÓGICO COMPLETO

LINGUAGENS:
• Python 3.10+

FRAMEWORKS:
• FastAPI (API REST assíncrona)
• Prefect (Workflow orchestration)

DATA PROCESSING:
• Xarray (Arrays multidimensionais)
• Dask (Parallel computing)
• Rasterio / GDAL (Geospatial raster)
• Shapely (Geometrias vetoriais)
• NumPy / Pandas (Manipulação numérica)

STORAGE:
• NetCDF4 (Formato científico padrão)
• GeoTIFF (Formato raster geoespacial)

VISUALIZATION:
• GeoServer (WMS server)
• Matplotlib / Cartopy (Plots estáticos)

INFRASTRUCTURE:
• Linux (Ubuntu/Debian)
• Systemd (Service management)
• Nginx (Reverse proxy)
• Docker (Containerization - opcional)

APIs EXTERNAS:
• Copernicus CDS (ERA5 data)
• NOAA AWS S3 (GLM data)
• Microsoft Planetary Computer STAC (Sentinel-2, MODIS)
• CHIRPS HTTP (Precipitation data)

---

MÉTRICAS DE PERFORMANCE ATUAIS

STARTUP:
• Shared Dask cluster: ~5 segundos
• Load all datasets: ~15 segundos
• Total startup time: ~20 segundos

QUERIES:
• Point timeseries (40 anos): 50-200ms
• Area statistics (1 ano, bbox): 200-500ms
• Polygon clip (complexo, 1 ano): 1-3 segundos
• WMS GetMap (cached): 10-50ms
• WMS GetMap (not cached): 200-800ms

STORAGE:
• Historical NetCDF per variable: 500MB - 2GB
• Daily GeoTIFF: 5-15 MB cada
• Total storage (7 variables, 40 years): ~150 GB

MEMORY:
• Dask workers: 4 × 6GB = 24 GB
• FastAPI process: ~500 MB
• GeoServer: ~2-4 GB
• Total RAM usage: ~30 GB

CPU:
• Normal load: 5-10%
• During processing: 80-100% (parallelizado)
• Dask workers: 2 threads each × 4 workers = 8 cores usados

---

PONTOS FORTES DA ARQUITETURA

1. **Escalabilidade horizontal**: Dask workers podem ser distribuídos em múltiplas máquinas

2. **Baixo custo operacional**: Todos os dados são gratuitos, infraestrutura mínima

3. **Padrões abertos**: NetCDF, GeoTIFF, WMS (OGC), OpenAPI

4. **Manutenibilidade**: Código modular, documentado, testado

5. **Performance**: Queries otimizadas com chunking estratégico

6. **Confiabilidade**: Fontes científicas oficiais, retry automático, logging completo

---

DESAFIOS TÉCNICOS SUPERADOS

1. **Memória**: Compartilhamento de Dask cluster economizou 50% de RAM

2. **Latência de queries**: Chunking time=1 otimizou séries temporais

3. **Sincronização dual-storage**: Pattern de check + process mantém consistência

4. **Async blocking**: asyncio.to_thread() resolveu problema de event loop

5. **GLM aggregation**: Binning espacial/temporal de 4300 arquivos/dia

6. **Cloud masking NDVI**: Compositing max reduz interferência de nuvens

7. **GeoServer time dimension**: ImageMosaic + indexer.properties

---

RECURSOS PARA DESENVOLVIMENTO

DOCUMENTAÇÃO:
• Xarray: https://docs.xarray.dev
• Dask: https://docs.dask.org
• FastAPI: https://fastapi.tiangolo.com
• Prefect: https://docs.prefect.io
• Rasterio: https://rasterio.readthedocs.io
• GeoServer: https://docs.geoserver.org

COMUNIDADES:
• Pangeo (Geoscience + Python): https://pangeo.io
• Xarray discussions: https://github.com/pydata/xarray/discussions
• GDAL mailing list: https://lists.osgeo.org/mailman/listinfo/gdal-dev

---

PRÓXIMOS PASSOS SUGERIDOS

CURTO PRAZO (1-2 meses):
1. Implementar índices de anomalia (SPI, desvios padronizados)
2. Adicionar radiação solar (ERA5 ssrd)
3. Criar alertas automáticos via webhook
4. Expandir testes unitários e integração

MÉDIO PRAZO (3-6 meses):
1. Implementar umidade do solo e evapotranspiração
2. Adicionar caching layer (Redis)
3. Dashboard web básico (React + Leaflet)
4. Documentação expandida com tutoriais

LONGO PRAZO (6-12 meses):
1. Task queue para processamentos pesados (Celery)
2. PostgreSQL + PostGIS para metadata
3. Machine learning para previsões (scikit-learn, PyTorch)
4. Expansão geográfica além de LatAm

---

CONTATO E COLABORAÇÃO

Repositório: /opt/geospatial_backend
Documentação: /opt/geospatial_backend/CLAUDE.md
API Docs: http://localhost:8000/docs
Dask Dashboard: http://localhost:8787

Equipe disponível para:
• Revisão técnica de código
• Debugging de queries complexas
• Otimização de performance
• Integração com sistemas existentes
• Treinamento técnico aprofundado

=================================================================================
FIM DA APRESENTAÇÃO TÉCNICA
=================================================================================
